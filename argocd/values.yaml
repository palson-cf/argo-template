## ArgoCD configuration
## Ref: https://github.com/argoproj/argo-cd
##
nameOverride: argocd
fullnameOverride: ""

global:
#   image:
    # repository: codefresh/argocd
    # tag: v1.7.6
    # imagePullPolicy: IfNotPresent

## Application controller
## Controller
controller:
  name: application-controller

  enableStatefulSet: true

  ## Argo controller commandline flags
  args:
    statusProcessors: "20"
    operationProcessors: "10"
    appResyncPeriod: "180"

  env:
  - name: ARGOCD_GPG_ENABLED
    value: "false"

  ## Argo controller log level
  logLevel: info

  ## Additional command line arguments to pass to argocd-controller
  ##
  extraArgs:
    - --repo-server-timeout-seconds
    - "300"

  resources:
    limits:
     cpu: 3
     memory: 4Gi
    requests:
     cpu: 3
     memory: 4Gi

  metrics:
    # enabled: true
    enabled: false
    service:
      annotations: {}
      labels: {}
      servicePort: 8082
    serviceMonitor:
      # enabled: true
      enabled: false
    #   selector:
    #     prometheus: kube-prometheus
    #   namespace: monitoring
      additionalLabels:
        release: cprom

    rules:
      # enabled: true
      enabled: false
      spec:
      - alert: ArgoAppMissing
        expr: |
          absent(argocd_app_info)
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "[ArgoCD] No reported applications"
          description: >
            ArgoCD has not reported any applications data for the past 15 minutes which
            means that it must be down or not functioning properly.  This needs to be
            resolved for this cloud to continue to maintain state.

      - alert: ArgoAppNotSynced
        expr: |
          argocd_app_info{sync_status!="Synced", project="e2e-envs" } == 1
        for: 12h
        labels:
          severity: warning
        annotations:
          summary: "{{$labels.name}} application not synchronized"
          description: >
            The application {{ $labels.name }} has not been synchronized for over
            12 hours which means that the state of this cloud has drifted away from the
            state inside Git.

      - alert: ArgoAppNotHealthy
        expr: |
          argocd_app_info{health_status!="Healthy", project="e2e-envs"} == 1
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "ArgoCD application {{$labels.name}} is not healthy"
          description: "The application has not been healthy for over 10 minutes.\nApplication: *{{$labels.name}}*\nCurrent status: *{{$labels.health_status}}*\nhttps://control-plane-dev.cf-op.com/argocd/applications/{{$labels.name}}"

      - alert: ArgoDown
        expr: up{job=~"argocd.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          message: "{{$labels.pod}} is down"
          summary: "{{$labels.pod}} is down"

      - alert: HpaMax
        expr: kube_hpa_status_desired_replicas / kube_hpa_spec_max_replicas >= 0.8
        labels:
          severity: critical
        annotations:
          message: |
            HPA pool {{ $labels.hpa }} is close to maximum capability
            please consider enlarging the pool
          summary: 'HPA pool {{ $labels.name }} hit the ceiling'

      - alert: ArgoMemoryUsage
        expr: |
          (
            container_memory_working_set_bytes {pod =~ "argocd-repo.*|argocd-server.*|argocd-application.*", container != "POD", image != ""}
            / on (pod, container, node)
            group_left()
            container_spec_memory_limit_bytes  {pod =~ "argocd-repo.*|argocd-server.*|argocd-application.*", container != "POD", image != ""}
          ) > 0.95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "ArgoCD pod {{$labels.pod}} used over 95% of memory limit"
          description: "ArgoCD pod {{$labels.pod}} used over 95% of memory limit"

      - alert: ArgoCpuUsage
        expr: |
          cpu:container:usage:ratio3m { pod =~ "argocd-.*" } > 0.95
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "ArgoCD pod {{$labels.pod}} used over 95% of CPU limit for 10 minutes"
          description: "ArgoCD pod {{$labels.pod}} used over 95% of CPU limit for 10 minutes"

      - expr: |
          sum by (pod, container) (rate(container_cpu_usage_seconds_total{image!="", container!="POD"}[3m]))
          /
          sum by (pod, container)
          (
            container_spec_cpu_quota{image!="", container!="POD"}
            /
            container_spec_cpu_period{image!="", container!="POD"}
          )
        record: cpu:container:usage:ratio3m

      - alert: ArgoCdBackupFailed
        expr: |
          kube_job_status_succeeded{namespace = "argocd", job_name =~ "argocd-backup.*" } != 1
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "ArgoCD backup job {{$labels.job_name}} failed"
          description: "ArgoCD backup job {{$labels.job_name}} failed"

#       selector:
        # prometheus: kube-prometheus
      namespace: monitoring
      additionalLabels:
        app: prometheus-operator
        release: cprom

  clusterAdminAccess:
    enabled: true

## Dex
dex:
  ## Additional volumeMounts to the controller main container.
#   volumeMounts:
    # - name: google
      # mountPath: /secrets
      # readOnly: true
    # - mountPath: /shared
#       name: static-files

  ## Additional volumes to the controller pod.
#   volumes:
    # - name: google
      # secret:
        # secretName: argocd-secret
        # items:
        # - key: dex.google.service-account
          # path: google-service-account.json
    # - emptyDir: {}
#       name: static-files

## Server
server:
  replicas: 2

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 8
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50

  extraArgs:
   - --rootpath
   - /argocd
   - --insecure
#    - --repo-server-timeout-seconds
#    - "300"
  ## Argo server log level

  clusterAdminAccess:
    enabled: false

  env:
  - name: ARGOCD_GPG_ENABLED
    value: "false"

  logLevel: info

  ## Node selectors and tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  nodeSelector: {}
  tolerations: []
  affinity: {}

  resources:
    limits:
     cpu: 1
     memory: 1Gi
    requests:
     cpu: 1
     memory: 1Gi


  ## Certificate configuration
  certificate:
    enabled: false
    domain: control-plane-dev.cf-op.com
    issuer:
      name: letsencrypt-prod
      kind: "ClusterIssuer"
    additionalHosts: []

  ## Server service configuration
  service:
    type: ClusterIP
    servicePortHttp: 80
    # servicePortHttps: 443
    # servicePortHttps: 80
    servicePortHttpName: http
    # servicePortHttpsName: https
    # servicePortHttpsName: http

  ## Server metrics service configuration
  metrics:
    # enabled: true
    enabled: false
    service:
      annotations: {}
      labels: {}
      servicePort: 8083
    serviceMonitor:
      # enabled: true
      enabled: false
    #   selector:
    #     prometheus: kube-prometheus
    #   namespace: monitoring
      additionalLabels:
        release: cprom

  serviceAccount:
    create: true
    name: argocd-server
    ## Annotations applied to created service account
    annotations: {}

  ingress:
    enabled: true
    annotations:
      certmanager.k8s.io/cluster-issuer: letsencrypt-prod
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/backend-protocol: HTTP
      # nginx.ingress.kubernetes.io/backend-protocol: HTTPS
      # nginx.ingress.kubernetes.io/ssl-passthrough: "true"

    hosts:
      - control-plane-dev.cf-op.com
    paths:
      - /argocd
    extraPaths:
    tls:
      - secretName: argocd-secret
        hosts:
          - control-plane-dev.cf-op.com
    https: false
    # https: true

  # dedicated ingess for gRPC as documented at
  # https://argoproj.github.io/argo-cd/operator-manual/ingress/
  ingressGrpc:
    # enabled: true
    enabled: false
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: GRPC
      kubernetes.io/ingress.class: nginx
      certmanager.k8s.io/cluster-issuer: letsencrypt-prod
      kubernetes.io/tls-acme: "true"

    labels: {}

    ## Argo Ingress.
    ## Hostnames must be provided if Ingress is enabled.
    ## Secrets must be manually created in the namespace
    ##
    hosts:
      - grpc.control-plane-dev.cf-op.com
    paths:
      - /
    extraPaths:
      []
    tls:
      - secretName: argocd-secret
        hosts:
          - grpc.control-plane-dev.cf-op.com
    https: true

  ## ArgoCD config
  ## reference https://github.com/argoproj/argo-cd/blob/master/docs/operator-manual/argocd-cm.yaml
  config:
    # Argo CD's externally facing base URL (optional). Required when configuring SSO
    url: https://control-plane-dev.cf-op.com/argocd
    # Argo CD instance label key
    application.instanceLabelKey: argocd.argoproj.io/instance

    admin.enabled: "false"

    # Codefresh integration
    accounts.codefresh-inc: apiKey
    accounts.codefresh-inc.enabled: "true"

#     dex.config: |
      # connectors:
      # - type: google
        # id: google
        # name: Google
        # config:
         # issuer: https://accounts.google.com
         # clientID: $dex.google.clientId
         # clientSecret: $dex.google.clientSecret
         # serviceAccountFilePath: /secrets/google-service-account.json
         # adminEmail: sharon.vendrov@codefresh.io
         # redirectURI: https://control-plane-dev.cf-op.com/argocd/api/dex/callback
         # groups:
           # - devops@codefresh.io
           # - dev@codefresh.io
           # - rnd@codefresh.io
#            - all@codefresh.io

  #   repositories: |
      # - name: codefresh-dev
        # type: helm
        # url: http://chartmuseum-dev.codefresh.io
      # - name: codefresh
        # type: helm
        # url: http://chartmuseum.codefresh.io
      # - name: stable
        # type: helm
        # url: https://charts.helm.sh/stable
    # repository.credentials: |
      # - url: git@github.com:codefresh-io
        # sshPrivateKeySecret:
          # name: argocd-repository-credentials
          # key: ssh-privatekey

  rbacConfig:
    policy.csv: |
      p, role:org-admin, applications, *, */*, allow
      p, role:org-admin, clusters, *, *, allow
      p, role:org-admin, repositories, *, *, allow
      p, role:org-admin, projects, *, *, allow
      g, devops@codefresh.io, role:org-admin

      p, role:org-viewer, applications, get, */*, allow
      p, role:org-viewer, clusters, get, *, allow
      p, role:org-viewer, repositories, get, *, allow
      p, role:org-viewer, projects, get, *, allow
      g, all@codefresh.io, role:org-viewer

      p, role:org-sync, applications, get, */*, allow
      p, role:org-sync, applications, sync, */*, allow
      p, role:org-sync, clusters, get, *, allow
      p, role:org-sync, repositories, get, *, allow
      p, role:org-sync, projects, get, *, allow
      g, codefresh-inc, role:org-sync
      g, dev@codefresh.io, role:org-sync
      g, rnd@codefresh.io, role:org-sync

  additionalProjects:
  - name: e2e-envs
    namespace: argocd
    additionalLabels: {}
    additionalAnnotations: {}
    description: Project for e2e tests
    sourceRepos:
    - '*'
    destinations:
    - namespace: '*'
      server: '*'
    clusterResourceWhitelist:
    - group: '*'
      kind: '*'
    roles: []
    orphanedResources: {}
    roles: []

## Repo Server
repoServer:
  name: repo-server

  replicas: 2

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 8
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50

  resources:
    limits:
      cpu: 4
      memory: 4Gi
    requests:
      cpu: 2
      memory: 4Gi

  ## Environment variables to pass to argocd-repo-server
  ##
  env:
  - name: ARGOCD_GPG_ENABLED
    value: "false"
  - name: ARGOCD_EXEC_TIMEOUT
    value: 600s
  - name: AWS_ACCESS_KEY_ID
    valueFrom:
      secretKeyRef:
        key: AWS_ACCESS_KEY_ID
        name: argocd-secret
  - name: AWS_SECRET_ACCESS_KEY
    valueFrom:
      secretKeyRef:
        key: AWS_SECRET_ACCESS_KEY
        name: argocd-secret

  ## Argo repoServer log level
  logLevel: info

  ## Node selectors and tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  nodeSelector: {}
  tolerations: []
  affinity: {}

  ## Repo server metrics service configuration
  metrics:
    # enabled: true
    enabled: false
    service:
      annotations: {}
      labels: {}
      servicePort: 8084
    serviceMonitor:
      enabled: false
      # enabled: true
    #   selector:
    #     prometheus: kube-prometheus
    #   namespace: monitoring
      additionalLabels:
        release: cprom

  ## Repo server service account
  ## If create is set to true, make sure to uncomment the name and update the rbac section below
  serviceAccount:
    create: false
    #  name: argocd-repo-server
    ## Annotations applied to created service account
    annotations: {}

# REDIS
##
redis-ha:
  # enabled: true
  enabled: false
  # Check the redis-ha chart for more properties
  exporter:
    enabled: true
  persistentVolume:
    enabled: true
  redis:
    masterGroupName: argocd
    config:
      save: "\"\""
  haproxy:
    enabled: true
    metrics:
      # enabled: true
      enabled: false
  image:
    tag: 5.0.8-alpine
